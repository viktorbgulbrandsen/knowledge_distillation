{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ede8c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: c:\\Users\\vikto\\Desktop\\mat-stk2011\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == \"experiments\":\n",
    "    os.chdir(cwd.parent)\n",
    "\n",
    "print(\"Working dir:\", Path.cwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a34252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.utils.seeds import seed_everything\n",
    "from src.utils.metrics import quadratic_weighted_kappa\n",
    "from src.utils.splits import get_stratified_folds\n",
    "from src.models.student import StudentConfig, StudentTree\n",
    "\n",
    "seed_everything(312)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a903db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:     (17307, 84)\n",
      "Hard labels:  [1 2 3 4 5 6]\n",
      "Soft targets: 1.0 to 5.89\n"
     ]
    }
   ],
   "source": [
    "spacy = np.load(\"data/cached_features_spacy.npz\")\n",
    "X = spacy[\"X\"]\n",
    "\n",
    "teacher = np.load(\"outputs/2026-02-20_16-53_teacher_cv/oof_predictions.npz\")\n",
    "y_true = teacher[\"y\"].astype(int)\n",
    "soft_probs = teacher[\"probs\"]  # (N, K)\n",
    "\n",
    "# expected-value soft targets (Approach 2)\n",
    "soft_targets = soft_probs @ np.arange(1, soft_probs.shape[1] + 1)\n",
    "\n",
    "print(\"Features:    \", X.shape)\n",
    "print(\"Hard labels: \", np.unique(y_true))\n",
    "print(\"Soft targets:\", soft_targets.min().round(2), \"to\", soft_targets.max().round(2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47c684e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft_probs shape: (17307, 6)\n",
      "\n",
      "First 5 soft label vectors:\n",
      "[[1.84676170e-01 6.26029432e-01 1.79044291e-01 9.66996700e-03\n",
      "  4.67232778e-04 1.12863927e-04]\n",
      " [2.85923388e-03 3.47079873e-01 6.44006014e-01 6.05355855e-03\n",
      "  1.29321995e-06 1.57624367e-08]\n",
      " [1.56955302e-05 4.81895462e-04 7.46353343e-02 8.51702869e-01\n",
      "  7.27309212e-02 4.33198467e-04]\n",
      " [1.11122546e-03 6.99999789e-03 1.55102417e-01 7.18026876e-01\n",
      "  1.15953557e-01 2.80598039e-03]\n",
      " [1.57676190e-01 5.20404935e-01 2.99094826e-01 2.24472620e-02\n",
      "  3.31540650e-04 4.52884342e-05]]\n",
      "\n",
      "Row sums (should be 1 if probabilities):\n",
      "[0.99999996 0.99999999 0.99999991 1.00000005 1.00000004]\n",
      "\n",
      "Example interpretation:\n",
      "Teacher distribution for sample 0:\n",
      "class 1: 0.185\n",
      "class 2: 0.626\n",
      "class 3: 0.179\n",
      "class 4: 0.010\n",
      "class 5: 0.000\n",
      "class 6: 0.000\n"
     ]
    }
   ],
   "source": [
    "print(\"soft_probs shape:\", soft_probs.shape)\n",
    "\n",
    "print(\"\\nFirst 5 soft label vectors:\")\n",
    "print(soft_probs[:5])\n",
    "\n",
    "print(\"\\nRow sums (should be 1 if probabilities):\")\n",
    "print(soft_probs[:5].sum(axis=1))\n",
    "\n",
    "print(\"\\nExample interpretation:\")\n",
    "i = 0\n",
    "print(\"Teacher distribution for sample 0:\")\n",
    "for k, p in enumerate(soft_probs[i], start=1):\n",
    "    print(f\"class {k}: {p:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc0963",
   "metadata": {},
   "source": [
    "# Interesting problem\n",
    "\n",
    "We consider a classification problem with labels\n",
    "$$\n",
    "y \\in \\mathcal C=\\{1,\\dots,K\\}.\n",
    "$$\n",
    "\n",
    "However, the teacher model provides for each input $x$ a probability vector\n",
    "$$\n",
    "p(x)=(p_1(x),\\dots,p_K(x)), \\qquad \\sum_{k=1}^K p_k(x)=1,\n",
    "$$\n",
    "rather than a single class label.\n",
    "\n",
    "Thus the supervision is a distribution over classes, not a categorical target.\n",
    "\n",
    "The problem is therefore:\n",
    "\n",
    "$$\n",
    "\\text{How do we train a classifier } f(x)\\in\\mathcal C\n",
    "\\text{ when the training targets are } p(x)\\in\\Delta^{K-1}?\n",
    "$$\n",
    "\n",
    "## Approach 1: Argmax reduction\n",
    "\n",
    "Discard the distribution entirely:\n",
    "$$\n",
    "\\hat y(x) = \\arg\\max_k\\, p_k(x).\n",
    "$$\n",
    "Reduces to standard classification. Loses all inter-class information —\n",
    "a teacher outputting $(0.05, 0.45, 0.50)$ produces the same target as $(0, 0, 1)$.\n",
    "\n",
    "Works with: any classifier (decision tree, SVM, anything).\n",
    "\n",
    "## Approach 2: Expected value regression\n",
    "\n",
    "Collapse the distribution to a scalar:\n",
    "$$\n",
    "\\tilde y(x) = \\sum_{k=1}^K k\\, p_k(x) = \\mathbb{E}_{Y \\sim p(x)}[Y].\n",
    "$$\n",
    "Train a regressor $g(x) \\in \\mathbb{R}$, classify by rounding $f(x) = \\mathrm{round}(g(x))$.\n",
    "\n",
    "Preserves ordinal information — uncertainty between 3 and 4 gives $\\tilde y \\approx 3.5$.\n",
    "But two very different distributions with the same mean produce identical targets.\n",
    "\n",
    "Works with: any regressor, including a single decision tree (minimises MSE natively).\n",
    "\n",
    "## Approach 3: Class-probability regression\n",
    "\n",
    "Fit $K$ separate regressors:\n",
    "$$\n",
    "g_k(x) \\approx p_k(x), \\qquad k = 1,\\dots,K,\n",
    "$$\n",
    "classify via $f(x) = \\arg\\max_k\\, g_k(x)$.\n",
    "\n",
    "Preserves the full distributional shape. Note that approach 2 is a linear\n",
    "projection of this: $\\tilde y = \\sum_k k\\, g_k(x)$. So this strictly generalises approach 2.\n",
    "\n",
    "Cost: $K$ models instead of one.\n",
    "\n",
    "Works with: any regressor, including $K$ separate decision trees.\n",
    "\n",
    "## Approach 4: Weighted mixture\n",
    "\n",
    "Interpolate between hard ground truth and soft teacher target:\n",
    "$$\n",
    "\\tilde y_\\alpha(x) = \\alpha\\, \\tilde y_{\\text{soft}}(x) + (1 - \\alpha)\\, y_{\\text{hard}}(x), \\qquad \\alpha \\in [0,1].\n",
    "$$\n",
    "Balances unbiased but noisy (hard) vs smooth but biased (soft) supervision.\n",
    "Optimal $\\alpha$ depends on teacher quality.\n",
    "\n",
    "Works with: any regressor (same as approach 2 but with mixed targets).\n",
    "\n",
    "## Approach 5: Direct KL minimisation (Frosst & Hinton, 2017)\n",
    "\n",
    "**Why approaches 1–4 exist as workarounds:** a standard decision tree splits\n",
    "greedily and never optimises a global loss. There is nowhere to plug in KL\n",
    "divergence. The above approaches are projections of the distributional problem\n",
    "into something a tree can handle natively.\n",
    "\n",
    "**Hinton's solution:** replace the standard tree with a *soft decision tree* that\n",
    "is fully differentiable and trained with gradient descent.\n",
    "\n",
    "Each internal node $i$ has a learned filter $w_i$ and bias $b_i$. The probability\n",
    "of taking the right branch is:\n",
    "$$\n",
    "p_i(x) = \\sigma\\bigl(\\beta(x w_i + b_i)\\bigr),\n",
    "$$\n",
    "where $\\beta$ is an inverse temperature. Each leaf $\\ell$ stores a learned distribution\n",
    "over classes:\n",
    "$$\n",
    "Q_k^\\ell = \\frac{\\exp(\\phi_k^\\ell)}{\\sum_{k'} \\exp(\\phi_{k'}^\\ell)}.\n",
    "$$\n",
    "The path probability to leaf $\\ell$ is the product of all branch probabilities along\n",
    "the path from the root. Because everything is differentiable, the loss directly\n",
    "minimises cross-entropy against the teacher's soft targets:\n",
    "$$\n",
    "L(x) = -\\log \\sum_{\\ell} P^\\ell(x) \\left( \\sum_k T_k \\log Q_k^\\ell \\right),\n",
    "$$\n",
    "where $T$ is the teacher's target distribution and $P^\\ell(x)$ is the path probability\n",
    "to leaf $\\ell$.\n",
    "\n",
    "This is the \"proper\" distillation into a tree — no projection, no workaround,\n",
    "the full distribution is matched via KL/cross-entropy.\n",
    "\n",
    "**Their results (MNIST):**\n",
    "- Soft tree on hard labels: 94.45%\n",
    "- Soft tree on teacher's soft labels: 96.76%\n",
    "- Teacher NN: 99.21%\n",
    "\n",
    "Distilled tree lands roughly halfway. Same pattern as our AES experiments.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebcc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup folds\n",
    "folds = get_stratified_folds(y_true, n_splits=5, seed=312)\n",
    "cfg = StudentConfig(max_depth=8, min_samples_leaf=10, random_state=312)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7b5b02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student (hard labels):  QWK = 0.6625\n"
     ]
    }
   ],
   "source": [
    "# get baseline\n",
    "\n",
    "oof_preds_hard = np.zeros(len(y_true), dtype=int)\n",
    "\n",
    "for i, (tr_idx, va_idx) in enumerate(folds):\n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X[tr_idx])\n",
    "    X_va = scaler.transform(X[va_idx])\n",
    "\n",
    "    tree = StudentTree(cfg, mode=\"classification\")\n",
    "    tree.fit(X_tr, y_true[tr_idx])\n",
    "    oof_preds_hard[va_idx] = tree.predict(X_va)\n",
    "\n",
    "qwk_hard = quadratic_weighted_kappa(y_true, oof_preds_hard)\n",
    "print(f\"Student (hard labels):  QWK = {qwk_hard:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b26a358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher argmax label distribution: (array([1, 2, 3, 4, 5, 6]), array([1210, 4575, 6452, 3941, 1042,   87]))\n",
      "========================================\n",
      "Hard labels (baseline): 0.6625\n",
      "Teacher argmax:         0.6882\n",
      "Delta vs hard:          +0.0256\n"
     ]
    }
   ],
   "source": [
    "# try Argmax reduction (teacher → hard labels)\n",
    "\n",
    "y_teacher = np.argmax(soft_probs, axis=1).astype(int) + 1  # convert to classes 1..K\n",
    "print(\"Teacher argmax label distribution:\", np.unique(y_teacher, return_counts=True))\n",
    "\n",
    "oof_preds_argmax = np.zeros(len(y_true), dtype=int)\n",
    "\n",
    "for tr_idx, va_idx in folds:\n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X[tr_idx])\n",
    "    X_va = scaler.transform(X[va_idx])\n",
    "\n",
    "    tree = StudentTree(cfg, mode=\"classification\")\n",
    "    tree.fit(X_tr, y_teacher[tr_idx])\n",
    "    oof_preds_argmax[va_idx] = tree.predict(X_va)\n",
    "\n",
    "qwk_argmax = quadratic_weighted_kappa(y_true, oof_preds_argmax)\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"Hard labels (baseline): {qwk_hard:.4f}\")\n",
    "print(f\"Teacher argmax:         {qwk_argmax:.4f}\")\n",
    "print(f\"Delta vs hard:          {qwk_argmax - qwk_hard:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
